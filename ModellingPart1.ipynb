{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Importing the Relevant Libraries](#Importing-the-Relevant-Libraries)\n",
    "- [Final EDA](#Final-EDA)\n",
    "- [Using Count Vectorizer](#Using-Count-Vectorizer)\n",
    "- [Mode1 1: Using The NaiveBias Bernoulli](#Mode1-1:-Using-The-NaiveBias-Bernoulli)\n",
    "- [Gridsearch + BernoulliNB function](#Gridsearch-+-BernoulliNB-function)\n",
    "- [Model 2: Using The Logistic Regression](#Model-2:-Using-The-Logistic-Regression)\n",
    "- [Gridsearch + Logistic Regression function](#Gridsearch-+-Logistic-Regression-function)\n",
    "- [Reapeating Process Using TF_IVF](#Reapeating-Process-Using-TF_IVF)\n",
    "- [Findings and Conclusions](#Findings-and-Conclusions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.13.2)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from mlxtend) (1.3.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.25.3)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from mlxtend) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.21.3)\n",
      "Requirement already satisfied: setuptools in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from mlxtend) (41.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from mlxtend) (1.17.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from pandas>=0.24.2->mlxtend) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from pandas>=0.24.2->mlxtend) (2019.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ikhyvicky/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->mlxtend) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/datatobemodelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>since 2015 using nasa hardware scientists and ...</td>\n",
       "      <td>askscience ama series we are experts on nasas ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wouldnt a pointed bow cut through the water be...</td>\n",
       "      <td>why arent the bows of submarines pointy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am always confused be centrifugal and centri...</td>\n",
       "      <td>do you weigh less at the equator because of ce...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if the universe is infused with dark matter wo...</td>\n",
       "      <td>could there possibly be black holes that forme...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i for example hold one arm straight to th...</td>\n",
       "      <td>does the brain send signals consistently to ke...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  since 2015 using nasa hardware scientists and ...   \n",
       "1  wouldnt a pointed bow cut through the water be...   \n",
       "2  i am always confused be centrifugal and centri...   \n",
       "3  if the universe is infused with dark matter wo...   \n",
       "4  when i for example hold one arm straight to th...   \n",
       "\n",
       "                                               title  target  \n",
       "0  askscience ama series we are experts on nasas ...     1.0  \n",
       "1            why arent the bows of submarines pointy     1.0  \n",
       "2  do you weigh less at the equator because of ce...     1.0  \n",
       "3  could there possibly be black holes that forme...     1.0  \n",
       "4  does the brain send signals consistently to ke...     1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1252, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "title     2\n",
       "target    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #idk why on cleaning notebook no nulls but upload here got nulls, so just remove since its small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['text','title']]\n",
    "y=df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'since 2015 using nasa hardware scientists and researchers have worked with astronauts on the international space station to conduct a series of experiments to grow harvest and eat a variety of crops in space with seeds sent from earth the most recent experiment has the iss crew growing mizuna mustard using two different light recipes and multiple harvests with the experiments final harvest scheduled for later this week this work builds upon decades of nasa and international research into growing plants in space\\n\\nthese experiments are advancing the knowledge required to successfully grow a large variety of crops on longduration missions such as a crewed mission to mars being able to crops grown in space provides many benefits including supplementing the astronauts packaged diet with essential nutrients and combating diet fatigue\\n\\nhere answering your questions are\\n\\n ralph fritsche space crop production project manager nasas kennedy space center\\n jess bunchek pseudonaut and associate scientist nasas kennedy space center\\n lashelle spencer research and development scientist nasas kennedy space center\\n jacob torres technical and horticultural scientist nasas kennedy space center \\n giola massa nasa veggie project lead nasas kennedy space center \\n\\nwe will see you at 230 pm eastern standard time 1930 ut ask us anything'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1250 entries, 0 to 1251\n",
      "Data columns (total 1 columns):\n",
      "target    1250 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 59.5 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stop_words.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by trying the naive biase first\n",
    "# Instantiate our CountVectorizer.\n",
    "#This step was inspired by what i saw online, involving strip-accents and justifying the ngrams\n",
    "cvec_text = CountVectorizer(stop_words=stopwords, strip_accents = 'ascii', ngram_range=(1, 6), min_df=.03, max_features=30)\n",
    "cvec_title = CountVectorizer(stop_words=stopwords, strip_accents = 'ascii', ngram_range=(1, 3), min_df=.01,max_features=30)\n",
    "\n",
    "# the assumption was that post texts with an ngram range of (1, 6) would both clean up noise and be more helpful than those with a range of (1, 1)\n",
    "# set a min_df to help clean up noise though it was gentler than previous ones\n",
    "# the assumption was that titles are likely more informative so setting an n-gram range of (1, 4) might provide even more helpful context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = cvec_text.fit_transform(X_train.text)\n",
    "X_train_title = cvec_title.fit_transform(X_train.title)\n",
    "\n",
    "X_test_text = cvec_text.transform(X_test.text)\n",
    "X_test_title = cvec_title.transform(X_test.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text_df = pd.DataFrame(X_train_text.todense(), columns=[x+'_text' for x in cvec_text.get_feature_names()])\n",
    "X_train_text_df.shape\n",
    "\n",
    "# creating a dataframe with my train post text and checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['actually_text', 'answer_text', 'body_text', 'day_text', 'did_text',\n",
       "       'different_text', 'does_text', 'dont_text', 'going_text', 'got_text',\n",
       "       'im_text', 'ive_text', 'just_text', 'know_text', 'like_text',\n",
       "       'make_text', 'point_text', 'question_text', 'really_text', 'said_text',\n",
       "       'say_text', 'says_text', 'space_text', 'sure_text', 'think_text',\n",
       "       'time_text', 'understand_text', 'water_text', 'way_text', 'years_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_title_df = pd.DataFrame(X_train_title.todense(), columns=[y+'_title' for y in cvec_title.get_feature_names()])\n",
    "X_train_title_df.shape\n",
    "\n",
    "# creating a dataframe with my train post titles and checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_text_df = pd.DataFrame(X_test_text.todense(), columns=[x+'_text' for x in cvec_text.get_feature_names()])\n",
    "X_test_text_df.shape\n",
    "\n",
    "# creating a dataframe with my test post text and checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_title_df = pd.DataFrame(X_test_title.todense(), columns=[y+'_title' for y in cvec_title.get_feature_names()])\n",
    "X_test_title_df.shape\n",
    "\n",
    "# creating a dataframe with my test post titles and checking the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecced_train_reddit_posts = pd.concat([X_train_text_df, X_train_title_df], axis=1)\n",
    "vecced_test_reddit_posts = pd.concat([X_test_text_df, X_test_title_df], axis=1)\n",
    "\n",
    "# concatenating my train text and titles back together as well as my test text and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 60)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_train_reddit_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually_text</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>body_text</th>\n",
       "      <th>day_text</th>\n",
       "      <th>did_text</th>\n",
       "      <th>different_text</th>\n",
       "      <th>does_text</th>\n",
       "      <th>dont_text</th>\n",
       "      <th>going_text</th>\n",
       "      <th>got_text</th>\n",
       "      <th>...</th>\n",
       "      <th>say_title</th>\n",
       "      <th>surface_title</th>\n",
       "      <th>temperature_title</th>\n",
       "      <th>time_title</th>\n",
       "      <th>use_title</th>\n",
       "      <th>walks_title</th>\n",
       "      <th>water_title</th>\n",
       "      <th>whats_title</th>\n",
       "      <th>wife_title</th>\n",
       "      <th>work_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actually_text  answer_text  body_text  day_text  did_text  \\\n",
       "0                0            0          0         0         0   \n",
       "1                0            0          0         0         0   \n",
       "2                0            0          0         0         0   \n",
       "3                0            0          0         0         0   \n",
       "4                0            0          0         0         0   \n",
       "..             ...          ...        ...       ...       ...   \n",
       "932              0            0          0         0         0   \n",
       "933              0            0          0         0         0   \n",
       "934              0            0          0         0         0   \n",
       "935              0            0          0         0         0   \n",
       "936              0            0          0         0         0   \n",
       "\n",
       "     different_text  does_text  dont_text  going_text  got_text  ...  \\\n",
       "0                 0          0          0           0         0  ...   \n",
       "1                 0          0          0           0         0  ...   \n",
       "2                 0          0          0           0         0  ...   \n",
       "3                 0          0          0           0         0  ...   \n",
       "4                 0          0          0           0         0  ...   \n",
       "..              ...        ...        ...         ...       ...  ...   \n",
       "932               0          0          0           0         0  ...   \n",
       "933               0          0          0           0         1  ...   \n",
       "934               0          2          0           0         1  ...   \n",
       "935               0          0          0           0         0  ...   \n",
       "936               0          0          0           0         0  ...   \n",
       "\n",
       "     say_title  surface_title  temperature_title  time_title  use_title  \\\n",
       "0            0              0                  0           0          0   \n",
       "1            0              0                  0           0          0   \n",
       "2            0              0                  0           0          0   \n",
       "3            0              0                  0           0          0   \n",
       "4            0              0                  0           0          0   \n",
       "..         ...            ...                ...         ...        ...   \n",
       "932          0              0                  0           0          0   \n",
       "933          0              0                  0           0          0   \n",
       "934          0              0                  1           0          0   \n",
       "935          0              0                  0           0          0   \n",
       "936          0              0                  0           0          0   \n",
       "\n",
       "     walks_title  water_title  whats_title  wife_title  work_title  \n",
       "0              0            0            0           0           0  \n",
       "1              0            0            0           0           0  \n",
       "2              0            0            0           0           0  \n",
       "3              0            0            0           0           0  \n",
       "4              0            0            0           0           1  \n",
       "..           ...          ...          ...         ...         ...  \n",
       "932            0            0            0           1           0  \n",
       "933            0            0            0           0           0  \n",
       "934            0            0            0           0           0  \n",
       "935            0            1            0           0           0  \n",
       "936            0            0            0           0           0  \n",
       "\n",
       "[937 rows x 60 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_train_reddit_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 60)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_test_reddit_posts.shape\n",
    "\n",
    "# checking the shape of my newly concatenated train dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode1 1: Using The NaiveBias Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our model!\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a function to instatiate gridsearch + bn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch + BernoulliNB function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_bn_models(model):\n",
    "    \n",
    "    if model == 'bn_1':\n",
    "        \n",
    "        bn_params = {               #from the documention and online research, we play around with the params\n",
    "            'fit_prior': [True],\n",
    "            'alpha': np.arange(0, 1, 0.1).tolist()} #np.arange(0, 1, 0.1).tolist()\n",
    "        \n",
    "        GS = GridSearchCV(BernoulliNB(),\n",
    "                        bn_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    elif model == 'bn_2':\n",
    "        \n",
    "         bn_params = {\n",
    "            'fit_prior': [False],\n",
    "            'alpha': np.arange(0, 1, 0.1).tolist()}\n",
    "        \n",
    "         GS = GridSearchCV(BernoulliNB(),\n",
    "                        bn_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "    GS.fit(vecced_train_reddit_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {GS.score(vecced_train_reddit_posts.values, y_train)}')\n",
    "    print(f'Test score = {GS.score(vecced_test_reddit_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = GS.predict(vecced_test_reddit_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    \n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print(f'Best params = {GS.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.8068303094983992\n",
      "Test score = 0.7763578274760383\n",
      "--------\n",
      "[[139  18]\n",
      " [ 52 104]]\n",
      "True Negatives: 139\n",
      "False Positives: 18\n",
      "False Negatives: 52\n",
      "True Positives: 104\n",
      "Best params = {'alpha': 0.2, 'fit_prior': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_bn_models('bn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Train score = 0.8068303094983992\n",
      "Test score = 0.7763578274760383\n",
      "--------\n",
      "[[139  18]\n",
      " [ 52 104]]\n",
      "True Negatives: 139\n",
      "False Positives: 18\n",
      "False Negatives: 52\n",
      "True Positives: 104\n",
      "Best params = {'alpha': 0.2, 'fit_prior': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  19 out of  50 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_bn_models('bn_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no difference between our 1st and 2nd naive model\n",
    "#ill just use the bn_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True negatives are things we correctly predict to be negative.\n",
    "<br>In our case, posts from subreddit askscience is assigned 1. A true negative means I correctly predicted a word comes from Jokes subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positives are things we falsely predict to be positive.\n",
    "<br>In our case, posts from subreddit askscience is assigned 1. A true positive means I incorrectly \n",
    "<br>predict a word comes from Askscience, but in actual fact it came from Jokes subreddit.\n",
    "<br>\n",
    "<br> In our case we got very few false positives. That means our model is quite legit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False negatives are things we falsely predict to be positive.\n",
    "<br>In our case, posts from subreddit askscience is assigned 1. A false negative means I incorrectly predicted a word comes from Jokes, but in reality its from Askscience.\n",
    "<br>\n",
    "<br>In our case we got abit more false negatives then false positives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Using The Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to instantiate the lr + gridsearchcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch + Logistic Regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_lr_models(model):\n",
    "    \n",
    "    if model == 'lr_1':\n",
    "        \n",
    "        lr_1_params = {\n",
    "            'penalty': ['l1'], #ridge regressiom\n",
    "            'C':  np.arange(1, 5, 0.1).tolist(), #[1, 1.5, 2, 2.5]\n",
    "            'class_weight': ['balanced'],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [42],\n",
    "            'solver': ['liblinear']}\n",
    "        \n",
    "        M = GridSearchCV(LogisticRegression(),\n",
    "                        lr_1_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    elif model == 'lr_2':\n",
    "        \n",
    "        lr_2_params = {\n",
    "            'penalty': ['l2'], #lasso regression\n",
    "            'C': np.arange(1, 5, 0.1).tolist(),\n",
    "            'class_weight': ['balanced'],\n",
    "            'warm_start': [True, False],            \n",
    "            'random_state': [42],\n",
    "            'solver': ['lbfgs', 'liblinear']}\n",
    "        \n",
    "        M = GridSearchCV(LogisticRegression(),\n",
    "                        lr_2_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "    M.fit(vecced_train_reddit_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {M.score(vecced_train_reddit_posts.values, y_train)}')\n",
    "    print(f'Test score = {M.score(vecced_test_reddit_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = M.predict(vecced_test_reddit_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print(f'Best params = {M.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.8324439701173959\n",
      "Test score = 0.7859424920127795\n",
      "--------\n",
      "[[139  18]\n",
      " [ 49 107]]\n",
      "True Negatives: 139\n",
      "False Positives: 18\n",
      "False Negatives: 49\n",
      "True Positives: 107\n",
      "Best params = {'C': 1.2000000000000002, 'class_weight': 'balanced', 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear', 'warm_start': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_lr_models('lr_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.8345784418356457\n",
      "Test score = 0.7827476038338658\n",
      "--------\n",
      "[[137  20]\n",
      " [ 48 108]]\n",
      "True Negatives: 137\n",
      "False Positives: 20\n",
      "False Negatives: 48\n",
      "True Positives: 108\n",
      "Best params = {'C': 2.6000000000000014, 'class_weight': 'balanced', 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear', 'warm_start': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_lr_models('lr_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our lr-1 performed better lr_2 so we would take score from this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing Bn_1 & Lr_2 ,\n",
    "#For Bn_1, the accuracyt of the test scores and train_score differed by 7%\n",
    "#For Lr_2, the accuracyt of the test scores and train_score differed by 4%\n",
    "#This suggest Lr_2 to be the more stable one.\n",
    "#Test_scores for both are similar, with Lr_2 edging ahead with an accuracy of 0.766\n",
    "#The score suggest that the Lr_2 is generalised enough to work elsewhere, not really overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What else can be done?\n",
    "#Testing out TF-IDF and subsequently doing MultinormialNB\n",
    "#So far i havent use stemming or lemmitizing. Maybe can consider doing that,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out TF-IDF\n",
    "#copy steps above and just change for TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reapeating Process Using TF_IVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_text = TfidfVectorizer(stop_words=stopwords, strip_accents = 'ascii', ngram_range=(1, 6), min_df=.03, max_features=30)\n",
    "tvec_title = TfidfVectorizer(stop_words=stopwords, strip_accents = 'ascii', ngram_range=(1, 3), min_df=.01,max_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text_tvec = tvec_text.fit_transform(X_train.text)\n",
    "X_train_title_tvec = tvec_title.fit_transform(X_train.title)\n",
    "\n",
    "X_test_text_tvec = tvec_text.transform(X_test.text)\n",
    "X_test_title_tvec = tvec_title.transform(X_test.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 30)\n",
      "(937, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train_text_tvec_df = pd.DataFrame(X_train_text_tvec.todense(), columns=[x+'_text' for x in cvec_text.get_feature_names()])\n",
    "print(X_train_text_tvec_df.shape)\n",
    "\n",
    "\n",
    "X_train_title_tvec_df = pd.DataFrame(X_train_title_tvec.todense(), columns=[y+'_title' for y in cvec_title.get_feature_names()])\n",
    "print(X_train_title_tvec_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313, 30)\n",
      "(313, 30)\n"
     ]
    }
   ],
   "source": [
    "X_test_text_tvec_df = pd.DataFrame(X_test_text_tvec.todense(), columns=[x+'_text' for x in cvec_text.get_feature_names()])\n",
    "print(X_test_text_tvec_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_test_title_tvec_df = pd.DataFrame(X_test_title.todense(), columns=[y+'_title' for y in cvec_title.get_feature_names()])\n",
    "print(X_test_title_tvec_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecced_train_reddit_tvec_posts = pd.concat([X_train_text_tvec_df, X_train_title_tvec_df], axis=1)\n",
    "vecced_test_reddit_tvec_posts = pd.concat([X_test_text_tvec_df, X_test_title_tvec_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually_text</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>body_text</th>\n",
       "      <th>day_text</th>\n",
       "      <th>did_text</th>\n",
       "      <th>different_text</th>\n",
       "      <th>does_text</th>\n",
       "      <th>dont_text</th>\n",
       "      <th>going_text</th>\n",
       "      <th>got_text</th>\n",
       "      <th>...</th>\n",
       "      <th>say_title</th>\n",
       "      <th>surface_title</th>\n",
       "      <th>temperature_title</th>\n",
       "      <th>time_title</th>\n",
       "      <th>use_title</th>\n",
       "      <th>walks_title</th>\n",
       "      <th>water_title</th>\n",
       "      <th>whats_title</th>\n",
       "      <th>wife_title</th>\n",
       "      <th>work_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.643239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actually_text  answer_text  body_text  day_text  did_text  \\\n",
       "0              0.0          0.0        0.0       0.0       0.0   \n",
       "1              0.0          0.0        0.0       0.0       0.0   \n",
       "2              0.0          0.0        0.0       0.0       0.0   \n",
       "3              0.0          0.0        0.0       0.0       0.0   \n",
       "4              0.0          0.0        0.0       0.0       0.0   \n",
       "..             ...          ...        ...       ...       ...   \n",
       "932            0.0          0.0        0.0       0.0       0.0   \n",
       "933            0.0          0.0        0.0       0.0       0.0   \n",
       "934            0.0          0.0        0.0       0.0       0.0   \n",
       "935            0.0          0.0        0.0       0.0       0.0   \n",
       "936            0.0          0.0        0.0       0.0       0.0   \n",
       "\n",
       "     different_text  does_text  dont_text  going_text  got_text  ...  \\\n",
       "0               0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "1               0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "2               0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "3               0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "4               0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "..              ...        ...        ...         ...       ...  ...   \n",
       "932             0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "933             0.0    0.00000        0.0         0.0  1.000000  ...   \n",
       "934             0.0    0.55405        0.0         0.0  0.342064  ...   \n",
       "935             0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "936             0.0    0.00000        0.0         0.0  0.000000  ...   \n",
       "\n",
       "     say_title  surface_title  temperature_title  time_title  use_title  \\\n",
       "0          0.0            0.0                0.0         0.0        0.0   \n",
       "1          0.0            0.0                0.0         0.0        0.0   \n",
       "2          0.0            0.0                0.0         0.0        0.0   \n",
       "3          0.0            0.0                0.0         0.0        0.0   \n",
       "4          0.0            0.0                0.0         0.0        0.0   \n",
       "..         ...            ...                ...         ...        ...   \n",
       "932        0.0            0.0                0.0         0.0        0.0   \n",
       "933        0.0            0.0                0.0         0.0        0.0   \n",
       "934        0.0            0.0                1.0         0.0        0.0   \n",
       "935        0.0            0.0                0.0         0.0        0.0   \n",
       "936        0.0            0.0                0.0         0.0        0.0   \n",
       "\n",
       "     walks_title  water_title  whats_title  wife_title  work_title  \n",
       "0            0.0     0.000000          0.0         0.0         0.0  \n",
       "1            0.0     0.000000          0.0         0.0         0.0  \n",
       "2            0.0     0.000000          0.0         0.0         0.0  \n",
       "3            0.0     0.000000          0.0         0.0         0.0  \n",
       "4            0.0     0.000000          0.0         0.0         1.0  \n",
       "..           ...          ...          ...         ...         ...  \n",
       "932          0.0     0.000000          0.0         1.0         0.0  \n",
       "933          0.0     0.000000          0.0         0.0         0.0  \n",
       "934          0.0     0.000000          0.0         0.0         0.0  \n",
       "935          0.0     0.643239          0.0         0.0         0.0  \n",
       "936          0.0     0.000000          0.0         0.0         0.0  \n",
       "\n",
       "[937 rows x 60 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_train_reddit_tvec_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_mn_models(model):\n",
    "    \n",
    "    if model == 'mn_1':\n",
    "        \n",
    "        mn_params = {               #from the documention and online research, we play around with the params\n",
    "            'fit_prior': [True],\n",
    "            'alpha': np.arange(0, 1, 0.1)}\n",
    "        \n",
    "        GS = GridSearchCV(MultinomialNB(),\n",
    "                        mn_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    elif model == 'mn_2':\n",
    "        \n",
    "         mn_params = {\n",
    "            'fit_prior': [False],\n",
    "            'alpha': np.arange(0, 1, 0.1)}\n",
    "        \n",
    "         GS = GridSearchCV(MultinomialNB(),\n",
    "                        mn_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "    GS.fit(vecced_train_reddit_tvec_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {GS.score(vecced_train_reddit_tvec_posts.values, y_train)}')\n",
    "    print(f'Test score = {GS.score(vecced_test_reddit_tvec_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = GS.predict(vecced_test_reddit_tvec_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print(f'Best params = {GS.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Train score = 0.7182497331910352\n",
      "Test score = 0.7092651757188498\n",
      "--------\n",
      "[[ 88  69]\n",
      " [ 22 134]]\n",
      "True Negatives: 88\n",
      "False Positives: 69\n",
      "False Negatives: 22\n",
      "True Positives: 134\n",
      "Best params = {'alpha': 0.9, 'fit_prior': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  50 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_mn_models('mn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  50 | elapsed:    0.0s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.8207043756670224\n",
      "Test score = 0.792332268370607\n",
      "--------\n",
      "[[130  27]\n",
      " [ 38 118]]\n",
      "True Negatives: 130\n",
      "False Positives: 27\n",
      "False Negatives: 38\n",
      "True Positives: 118\n",
      "Best params = {'alpha': 0.7000000000000001, 'fit_prior': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_mn_models('mn_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multinomial model 2 performed better , having more TPs and TRUE negatives. Has a better test accuracy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_lr_models(model):\n",
    "    \n",
    "    if model == 'lr_1':\n",
    "        \n",
    "        lr_1_params = {\n",
    "            'penalty': ['l1'], #ridge regressiom\n",
    "            'C': np.arange(1, 5, 0.1).tolist(),\n",
    "            'class_weight': ['balanced'],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [42],\n",
    "            'solver': ['liblinear']}\n",
    "        \n",
    "        M = GridSearchCV(LogisticRegression(),\n",
    "                        lr_1_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    elif model == 'lr_2':\n",
    "        \n",
    "        lr_2_params = {\n",
    "            'penalty': ['l2'], #lasso regression\n",
    "            'C': np.arange(1, 5, 0.1).tolist(),\n",
    "            'class_weight': ['balanced'],\n",
    "            'warm_start': [True, False],            \n",
    "            'random_state': [42],\n",
    "            'solver': ['lbfgs', 'liblinear']}\n",
    "        \n",
    "        M = GridSearchCV(LogisticRegression(),\n",
    "                        lr_2_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "\n",
    "    M.fit(vecced_train_reddit_tvec_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {M.score(vecced_train_reddit_tvec_posts.values, y_train)}')\n",
    "    print(f'Test score = {M.score(vecced_test_reddit_tvec_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = M.predict(vecced_test_reddit_tvec_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print(f'Best params = {M.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.8537886872998933\n",
      "Test score = 0.7955271565495208\n",
      "--------\n",
      "[[133  24]\n",
      " [ 40 116]]\n",
      "True Negatives: 133\n",
      "False Positives: 24\n",
      "False Negatives: 40\n",
      "True Positives: 116\n",
      "Best params = {'C': 3.200000000000002, 'class_weight': 'balanced', 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear', 'warm_start': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_lr_models('lr_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Train score = 0.8495197438633938\n",
      "Test score = 0.7987220447284346\n",
      "--------\n",
      "[[133  24]\n",
      " [ 39 117]]\n",
      "True Negatives: 133\n",
      "False Positives: 24\n",
      "False Negatives: 39\n",
      "True Positives: 117\n",
      "Best params = {'C': 1.8000000000000007, 'class_weight': 'balanced', 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear', 'warm_start': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_lr_models('lr_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log reg 1 & 2 performed similarly  regardless of params, hence we can pick either one to compare with our mn model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing all models it was shown that our TF-IDF with MultinomialNB model has the best performance based on \n",
    "<br>1) A more stable accuracy score from train to test \n",
    "<br>2) An accuuracy that is not too high, suggesting that our model is not overfit, and quite generalised\n",
    "\n",
    "<br>We are predicting words from the Jokes subreddit 42% of the time!\n",
    "<br>Misclassifying words only 20% of the time.\n",
    "\n",
    "<br>Below we plotted the ROC curve to show our models effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('1-Specificity')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.7000000000000001, class_prior=None, fit_prior=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB(fit_prior=False,alpha=0.7000000000000001)\n",
    "model.fit(vecced_train_reddit_tvec_posts.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(vecced_test_reddit_tvec_posts)\n",
    "probs = probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.86\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test, probs)\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our ROC curve, the AUC for this model is 0.86. The closer the AUC is to one, the more effective a model it is.\n",
    "<br>As such, model does quite a good job of distinguishing the positive and the negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, our model proved to be quite effective in classifying words from both subreddits. \n",
    "This model relies on both the words in sub-text as well as the title.\n",
    "future works could maybe delve into looking into building separate models for sub-text as well as titles.\n",
    "Moreover, these models have to constantly be retrained, due to the emergence of new urban words, abbreviations. They may or may not be relaterd to ASKSCIENCE, and simply throwing them out will just make future models less accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
